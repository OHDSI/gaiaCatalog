#########
# GDSC ETL is performed in two steps: shell script to ETL data into postgres
# and then additional transformations with SQL in postGIS (if needed).
####

#########
# Step 1 - bash script (pseudo code)
####

# create directory structure and move into it
mkdir -p /data/tz_1984_copernicus_avg_temp/{download,etl} && cd /data/tz_1984_copernicus_avg_temp

# set update flag based on last update and update frequency
do_update=0 if date() < last_update + update_frequency else do_update = 1

# clone a git repo and run the code# save netcdf as tif
cd download
gdal_translate NETCDF:tz_1984_copernicus_avg_temp.nc:2m_temperature tz_1984_copernicus_avg_temp.tif
cd ..


#########
# Step 2 - SQL script (pseudo code):
####

# load raster into postGIS with:
    raster2pgsql -s 4326 -d -C -I -t auto tz_1984_copernicus_avg_temp.tif -F tz_1984_copernicus_avg_temp > load_raster.sql
psql <connection options> < load_raster.sql
rm load_raster.sql

# Move into corrrect directory and create derivative directory in data package on osgeo
cd /data/tz_1984_copernicus_avg_temp/
mkdir -p derived

export PGPASSWORD=$(cat $POSTGRES_PASSWORD_FILE)
# Move into correct directory
cd /data/tz_1984_copernicus_avg_temp/

export PGPASSWORD=$(cat $POSTGRES_PASSWORD_FILE)

